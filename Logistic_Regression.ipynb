{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "#### Logistic Regression:\n",
        "\n",
        "Logistic Regression is a **supervised machine learning algorithm** used for **classification tasks**, particularly **binary classification** (such as yes/no, spam/ham, pass/fail).\n",
        "\n",
        "* It predicts the **probability** that a given input belongs to a particular category.\n",
        "* The output is passed through a **sigmoid function**, which converts it into a probability between 0 and 1.\n",
        "\n",
        "**Equation**:\n",
        "\n",
        "$$\n",
        "P(Y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n)}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Linear Regression:\n",
        "\n",
        "Linear Regression is a **supervised learning algorithm** used for **regression tasks**, where the output is a **continuous** numeric value.\n",
        "\n",
        "**Equation**:\n",
        "\n",
        "$$\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_n X_n + \\epsilon\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Feature              | Logistic Regression                            | Linear Regression                          |\n",
        "| -------------------- | ---------------------------------------------- | ------------------------------------------ |\n",
        "| Purpose              | Classification                                 | Regression                                 |\n",
        "| Output               | Probability (0 to 1)                           | Continuous value                           |\n",
        "| Target Variable      | Categorical (usually binary: 0 or 1)           | Numeric/Continuous                         |\n",
        "| Function Used        | Sigmoid (logistic) function                    | Linear function                            |\n",
        "| Best suited for      | Email spam detection, disease prediction, etc. | Price prediction, demand forecasting, etc. |\n",
        "| Prediction Threshold | Usually 0.5 (if probability ≥ 0.5 → class = 1) | Not applicable                             |\n"
      ],
      "metadata": {
        "id": "GbZfiLuKQZpA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is the Mathematical Equation of Logistic Regression?\n",
        "\n",
        "The core idea behind Logistic Regression is to model the **probability** that the output $Y$ belongs to class 1 (e.g., success, yes, positive) given the input features $X$. Instead of predicting $Y$ directly, logistic regression predicts the **log-odds** (also called the logit), which is then converted into a probability using the **sigmoid function**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Step-by-step breakdown:\n",
        "\n",
        "1. **Linear combination of inputs**:\n",
        "\n",
        "$$\n",
        "z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "2. **Sigmoid (logistic) function**:\n",
        "\n",
        "$$\n",
        "P(Y = 1 | X) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
        "$$\n",
        "\n",
        "This function outputs a value between 0 and 1, which can be interpreted as a **probability**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Log-odds (logit) form:\n",
        "\n",
        "Alternatively, the model can be expressed in terms of **log-odds**:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{P(Y=1|X)}{1 - P(Y=1|X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "This means logistic regression is modeling the **logarithm of the odds** of the positive class as a linear function of the inputs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1ff1Su_QZln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Why Do We Use the Sigmoid Function in Logistic Regression?\n",
        "\n",
        "The **sigmoid function** is used in logistic regression to map any real-valued number into a range between **0 and 1**, making it ideal for **modeling probabilities**.\n",
        "\n",
        "---\n",
        "\n",
        "#### The sigmoid function is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where $z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n$\n",
        "\n",
        "---\n",
        "\n",
        "### Reasons for Using the Sigmoid Function:\n",
        "\n",
        "| Reason                        | Explanation                                                                  |\n",
        "| ----------------------------- | ---------------------------------------------------------------------------- |\n",
        "| **Probability Output**        | Outputs a value between 0 and 1, making it interpretable as a probability.   |\n",
        "| **Smooth Gradient**           | Enables gradient-based optimization (like gradient descent) to work well.    |\n",
        "| **Decision Boundary**         | Allows setting a threshold (e.g., 0.5) to classify inputs into classes.      |\n",
        "| **Differentiability**         | It is differentiable everywhere, which is essential for training the model.  |\n",
        "| **Maps Linear to Non-linear** | Converts linear output into a non-linear S-curve, useful for classification. |\n",
        "\n",
        "---\n",
        "\n",
        "### Visual Insight:\n",
        "\n",
        "* For large negative values of $z$, sigmoid outputs values close to **0**.\n",
        "* For large positive values of $z$, it outputs values close to **1**.\n",
        "* At $z = 0$, sigmoid outputs **0.5** (the threshold for classification).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3cec2TQHQZgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What is the Cost Function of Logistic Regression?\n",
        "\n",
        "In **logistic regression**, we use a cost function based on **maximum likelihood estimation**, and the most common form is the **log loss** or **binary cross-entropy**.\n",
        "\n",
        "---\n",
        "\n",
        "### Cost Function (Log Loss):\n",
        "\n",
        "For a single training example:\n",
        "\n",
        "$$\n",
        "\\text{Cost}(h_\\theta(x), y) = -y \\log(h_\\theta(x)) - (1 - y) \\log(1 - h_\\theta(x))\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}$ is the sigmoid output (predicted probability).\n",
        "* $y$ is the actual label (0 or 1).\n",
        "\n",
        "---\n",
        "\n",
        "### For the entire dataset (with $m$ examples):\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Why This Cost Function?\n",
        "\n",
        "* It penalizes wrong predictions **more heavily** than correct ones.\n",
        "* It is **convex**, which means it has a single global minimum, allowing efficient optimization using gradient descent.\n",
        "* It ensures that probabilities close to the true labels get **low loss**, while poor predictions result in **high loss**.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Properties:\n",
        "\n",
        "| Property             | Explanation                                         |\n",
        "| -------------------- | --------------------------------------------------- |\n",
        "| Convex Function      | Guarantees a unique minimum for optimization.       |\n",
        "| Probabilistic Output | Works well with the sigmoid output (0–1).           |\n",
        "| Logarithmic Penalty  | Strongly penalizes confident but wrong predictions. |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZTZbESgaRNnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. What is Regularization in Logistic Regression? Why is it Needed?\n",
        "\n",
        "#### **What is Regularization?**\n",
        "\n",
        "Regularization is a technique used in logistic regression (and other models) to **prevent overfitting** by adding a **penalty term** to the cost function. This penalty discourages the model from fitting the training data too closely by controlling the **magnitude of the model's coefficients**.\n",
        "\n",
        "---\n",
        "\n",
        "### Why is Regularization Needed?\n",
        "\n",
        "Without regularization:\n",
        "\n",
        "* The model might learn **noise or random fluctuations** in the training data.\n",
        "* This results in **overfitting**, where the model performs well on training data but poorly on unseen (test) data.\n",
        "* Large coefficients can lead to **unstable predictions**, especially with small changes in inputs.\n",
        "\n",
        "Regularization helps to:\n",
        "\n",
        "* Simplify the model.\n",
        "* Improve **generalization** to new data.\n",
        "* Maintain **stability** in predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### Types of Regularization in Logistic Regression:\n",
        "\n",
        "| Type            | Penalty Term              | Description                                                                          |   |                                                                                    |\n",
        "| --------------- | ------------------------- | ------------------------------------------------------------------------------------ | - | ---------------------------------------------------------------------------------- |\n",
        "| **L1 (Lasso)**  | ( \\lambda \\sum            | \\theta\\_j                                                                            | ) | Encourages sparsity; can shrink some coefficients to **zero** (feature selection). |\n",
        "| **L2 (Ridge)**  | $\\lambda \\sum \\theta_j^2$ | Penalizes large weights; helps reduce model complexity without eliminating features. |   |                                                                                    |\n",
        "| **Elastic Net** | Combination of L1 and L2  | Combines benefits of both.                                                           |   |                                                                                    |\n",
        "\n",
        "---\n",
        "\n",
        "### Regularized Cost Function Example (L2):\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(h_\\theta(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "Here, $\\lambda$ controls the **strength of the penalty** (larger $\\lambda$ = stronger regularization).\n",
        "\n"
      ],
      "metadata": {
        "id": "I8nOK9u1Rbi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 6. Difference Between Lasso, Ridge, and Elastic Net Regression\n",
        "\n",
        "These are **regularization techniques** used in linear and logistic regression to **prevent overfitting** by adding a penalty term to the cost function. They differ in how they apply this penalty.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Ridge Regression (L2 Regularization)\n",
        "\n",
        "* **Penalty Term**: Adds the sum of **squared** coefficients to the loss function.\n",
        "\n",
        "  $$\n",
        "  \\text{Penalty} = \\lambda \\sum_{j=1}^n \\theta_j^2\n",
        "  $$\n",
        "\n",
        "* **Effect**: Shrinks coefficients toward zero, but **does not eliminate** any (none become exactly zero).\n",
        "\n",
        "* **Use Case**: When **all features are relevant**, and you want to reduce overfitting without removing features.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Lasso Regression (L1 Regularization)\n",
        "\n",
        "* **Penalty Term**: Adds the sum of the **absolute values** of the coefficients.\n",
        "\n",
        "  $$\n",
        "  \\text{Penalty} = \\lambda \\sum_{j=1}^n |\\theta_j|\n",
        "  $$\n",
        "\n",
        "* **Effect**: Can shrink some coefficients **exactly to zero**, effectively performing **feature selection**.\n",
        "\n",
        "* **Use Case**: When you believe that **only a few features** are significant and want a simpler model.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Elastic Net Regression (Combination of L1 and L2)\n",
        "\n",
        "* **Penalty Term**: Combines both L1 and L2 regularization.\n",
        "\n",
        "  $$\n",
        "  \\text{Penalty} = \\lambda_1 \\sum_{j=1}^n |\\theta_j| + \\lambda_2 \\sum_{j=1}^n \\theta_j^2\n",
        "  $$\n",
        "\n",
        "* **Effect**: Encourages sparsity like Lasso and maintains group behavior of correlated features like Ridge.\n",
        "\n",
        "* **Use Case**: When you need **both variable selection and regularization**, especially if features are correlated.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Feature                  | Ridge (L2)                  | Lasso (L1)                   | Elastic Net (L1 + L2)          |\n",
        "| ------------------------ | --------------------------- | ---------------------------- | ------------------------------ |\n",
        "| Penalty Term             | Sum of squared coefficients | Sum of absolute coefficients | Combination of both            |\n",
        "| Coefficient Shrinkage    | Yes                         | Yes                          | Yes                            |\n",
        "| Coefficients Set to Zero | No                          | Yes (some)                   | Yes (some)                     |\n",
        "| Feature Selection        | No                          | Yes                          | Yes                            |\n",
        "| Best Use Case            | Many relevant features      | Few relevant features        | Correlated or grouped features |\n",
        "\n"
      ],
      "metadata": {
        "id": "9eTMPgF9Rpcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. When Should We Use Elastic Net Instead of Lasso or Ridge?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 1. **You have many correlated features**\n",
        "\n",
        "* **Lasso** tends to arbitrarily select one feature and ignore others, which can be unstable when predictors are highly correlated.\n",
        "* **Elastic Net** overcomes this by allowing **grouping**: it tends to select **multiple correlated features together**.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **You want both feature selection and regularization**\n",
        "\n",
        "* **Lasso** can eliminate irrelevant features (set their coefficients to zero), but may underperform when features are correlated.\n",
        "* **Ridge** keeps all features but doesn't perform feature selection.\n",
        "* **Elastic Net** combines both benefits: it **selects important variables** and **shrinks** others to control overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Lasso fails to select the correct variables**\n",
        "\n",
        "* If Lasso is **too aggressive**, especially in datasets where the number of predictors is greater than the number of observations (high-dimensional data), it may miss important variables.\n",
        "* Elastic Net is more stable in such cases.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **You are dealing with high-dimensional data**\n",
        "\n",
        "* When $p > n$ (number of predictors > number of observations), **Elastic Net** performs better than Lasso or Ridge alone.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "| Situation                       | Recommended Method   |\n",
        "| ------------------------------- | -------------------- |\n",
        "| Many correlated features        | Elastic Net          |\n",
        "| Need for feature selection      | Lasso or Elastic Net |\n",
        "| Need to retain all features     | Ridge                |\n",
        "| High-dimensional data (p > n)   | Elastic Net          |\n",
        "| Lasso underperforms or unstable | Elastic Net          |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HuKIA6hlSAOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. What is the Impact of the Regularization Parameter (λ) in Logistic Regression?\n",
        "\n",
        "The **regularization parameter $\\lambda$** (lambda) controls the **strength of the penalty** applied to the model's coefficients in logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Impacts of λ:\n",
        "\n",
        "#### 1. **Controls Model Complexity**\n",
        "\n",
        "* **Large λ** → Strong penalty → Coefficients are shrunk more → Model becomes simpler\n",
        "* **Small λ** → Weak penalty → Coefficients are barely shrunk → Model becomes more complex\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Affects Overfitting and Underfitting**\n",
        "\n",
        "| λ Value            | Model Behavior     | Risk                |\n",
        "| ------------------ | ------------------ | ------------------- |\n",
        "| **λ = 0**          | No regularization  | Overfitting         |\n",
        "| **λ is too large** | Too much shrinkage | Underfitting        |\n",
        "| **λ is optimal**   | Balanced fit       | Good generalization |\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Influences Feature Selection (L1 only)**\n",
        "\n",
        "* In **Lasso (L1) regularization**, a larger λ can set more coefficients exactly to **zero**, removing unimportant features.\n",
        "* In **Ridge (L2)**, all coefficients are shrunk but remain non-zero.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Bias-Variance Trade-off**\n",
        "\n",
        "* **High λ** increases **bias**, reduces **variance** (simpler but less accurate model).\n",
        "* **Low λ** reduces **bias**, increases **variance** (more accurate on training data but may not generalize well).\n",
        "\n",
        "---\n",
        "\n",
        "### In Summary:\n",
        "\n",
        "| λ Value         | Regularization Strength | Model Fit     | Common Outcome      |\n",
        "| --------------- | ----------------------- | ------------- | ------------------- |\n",
        "| Small (≈ 0)     | Weak                    | Very flexible | Overfitting risk    |\n",
        "| Large           | Strong                  | Too rigid     | Underfitting risk   |\n",
        "| Optimal (tuned) | Balanced                | Good          | Best generalization |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9rwh17YmSUrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What Are the Key Assumptions of Logistic Regression?\n",
        "\n",
        "Logistic Regression, while more flexible than linear regression, still relies on several important assumptions to ensure valid results and reliable predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Binary (or Multiclass) Outcome**\n",
        "\n",
        "* The **dependent variable** should be **binary** (0 or 1) for binary logistic regression.\n",
        "* For more than two categories, **multinomial logistic regression** is used.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Linearity of Logit**\n",
        "\n",
        "* Logistic regression **assumes a linear relationship between the independent variables and the log-odds** (logit) of the dependent variable.\n",
        "  That is:\n",
        "\n",
        "  $$\n",
        "  \\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "  $$\n",
        "\n",
        "  Not between predictors and the probability directly, but with the **log-odds**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **No or Little Multicollinearity**\n",
        "\n",
        "* **Independent variables should not be highly correlated** with each other.\n",
        "* High multicollinearity distorts the estimation of coefficients.\n",
        "* Can be checked using **Variance Inflation Factor (VIF)**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Independence of Observations**\n",
        "\n",
        "* Observations should be **independent** from one another (no autocorrelation).\n",
        "* Logistic regression is **not suitable for time-series or clustered data** unless modified (e.g., using mixed models).\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Large Sample Size**\n",
        "\n",
        "* Logistic regression performs better with **a large dataset**, especially when the event (1's) is rare.\n",
        "* Helps in obtaining stable and reliable estimates.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Minimal Outliers in Predictors**\n",
        "\n",
        "* Outliers in independent variables can significantly impact the model’s coefficients.\n",
        "* It’s recommended to detect and treat them properly.\n",
        "\n",
        "---\n",
        "\n",
        "### Optional (for interpretability):\n",
        "\n",
        "* **Homoscedasticity** is **not required**.\n",
        "* **Normality of predictors** is **not required**, unlike in linear regression.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Assumption                        | Required in Logistic Regression |\n",
        "| --------------------------------- | ------------------------------- |\n",
        "| Binary outcome                    | Yes                             |\n",
        "| Linearity with log-odds           | Yes                             |\n",
        "| No multicollinearity              | Yes                             |\n",
        "| Independent observations          | Yes                             |\n",
        "| Large sample size                 | Recommended                     |\n",
        "| Homoscedasticity (equal variance) | No                              |\n",
        "| Normality of predictors           | No                              |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_cD55YzYShgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. What Are Some Alternatives to Logistic Regression for Classification Tasks?\n",
        "\n",
        "Logistic Regression is a simple and effective model, but there are many other classification algorithms that may perform better, especially with **non-linear**, **high-dimensional**, or **complex** datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Alternatives:\n",
        "\n",
        "#### 1. **Decision Trees**\n",
        "\n",
        "* Rule-based model that splits data into branches.\n",
        "* Easy to interpret, handles non-linearity.\n",
        "* Can overfit if not pruned.\n",
        "\n",
        "#### 2. **Random Forest**\n",
        "\n",
        "* Ensemble of multiple decision trees (bagging).\n",
        "* More accurate and robust than a single tree.\n",
        "* Handles both categorical and numerical data well.\n",
        "\n",
        "#### 3. **Support Vector Machines (SVM)**\n",
        "\n",
        "* Constructs hyperplanes to separate classes.\n",
        "* Effective in high-dimensional spaces.\n",
        "* Good with margin-based classification.\n",
        "\n",
        "#### 4. **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "* Instance-based learning; classifies based on closest training examples.\n",
        "* Simple but can be slow and sensitive to irrelevant features.\n",
        "\n",
        "#### 5. **Naive Bayes**\n",
        "\n",
        "* Based on Bayes’ Theorem with strong independence assumptions.\n",
        "* Fast and effective with text classification (e.g., spam detection).\n",
        "\n",
        "#### 6. **Gradient Boosting Machines (GBM, XGBoost, LightGBM, CatBoost)**\n",
        "\n",
        "* Build models sequentially to fix previous errors.\n",
        "* High accuracy on complex datasets.\n",
        "* Requires tuning but often outperforms logistic regression.\n",
        "\n",
        "#### 7. **Neural Networks**\n",
        "\n",
        "* Highly flexible, suitable for large and complex datasets.\n",
        "* Can model complex non-linear relationships.\n",
        "* Requires more data and computation.\n",
        "\n",
        "---\n",
        "\n",
        "### When to Consider Alternatives:\n",
        "\n",
        "| Situation                      | Better Alternatives                  |\n",
        "| ------------------------------ | ------------------------------------ |\n",
        "| Non-linear decision boundaries | SVM, Decision Trees, Neural Networks |\n",
        "| High-dimensional data          | SVM, Naive Bayes                     |\n",
        "| Complex relationships          | Random Forest, Gradient Boosting     |\n",
        "| Large datasets with patterns   | XGBoost, LightGBM                    |\n",
        "| Low computational resources    | Naive Bayes, Logistic Regression     |\n",
        "| Interpretability is essential  | Logistic Regression, Decision Trees  |\n",
        "\n"
      ],
      "metadata": {
        "id": "7nS19TKqSvFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. What Are Classification Evaluation Metrics?\n",
        "\n",
        "Classification evaluation metrics are used to **assess the performance** of a classification model. These metrics help determine how well the model predicts the correct class labels on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Classification Metrics:\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "\n",
        "* The ratio of correctly predicted instances to total instances.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "* **Best for**: balanced datasets.\n",
        "* **Limitation**: misleading with imbalanced classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Precision**\n",
        "\n",
        "* The proportion of true positives among all predicted positives.\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "* **High precision**: fewer false positives.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "* The proportion of actual positives correctly identified.\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "* **High recall**: fewer false negatives.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **F1 Score**\n",
        "\n",
        "* Harmonic mean of precision and recall.\n",
        "\n",
        "$$\n",
        "\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "* **Best for**: imbalanced datasets where both false positives and false negatives matter.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Confusion Matrix**\n",
        "\n",
        "* A table showing counts of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\n",
        "* Gives a full picture of prediction performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **ROC Curve and AUC (Area Under Curve)**\n",
        "\n",
        "* **ROC Curve** plots true positive rate vs. false positive rate.\n",
        "* **AUC** quantifies overall ability to discriminate between classes.\n",
        "* AUC = 1 → perfect model; AUC = 0.5 → random guessing.\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Log Loss (Cross-Entropy Loss)**\n",
        "\n",
        "* Measures the uncertainty of predictions based on probability.\n",
        "* Lower log loss indicates better probability estimates.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Metric           | Best For                       | Notes                                |\n",
        "| ---------------- | ------------------------------ | ------------------------------------ |\n",
        "| Accuracy         | Balanced classes               | Misleading on imbalanced data        |\n",
        "| Precision        | Minimizing false positives     | Important in spam or fraud detection |\n",
        "| Recall           | Minimizing false negatives     | Important in medical diagnostics     |\n",
        "| F1 Score         | Balancing precision and recall | Good for uneven class distribution   |\n",
        "| AUC-ROC          | Model's ranking ability        | Useful for comparing classifiers     |\n",
        "| Confusion Matrix | Complete classification report | Used to compute all metrics above    |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F0dplhzSS6Jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. How Does Class Imbalance Affect Logistic Regression?\n",
        "\n",
        "**Class imbalance** occurs when one class significantly outnumbers the other (e.g., 95% class 0 and 5% class 1). This can cause **serious problems** for logistic regression and other classification models.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Effects of Class Imbalance:\n",
        "\n",
        "#### 1. **Misleading Accuracy**\n",
        "\n",
        "* Logistic regression may **predict only the majority class** to achieve high accuracy.\n",
        "* For example, if 95% of samples are class 0, predicting class 0 for every input gives 95% accuracy—but fails to detect the minority class (class 1).\n",
        "\n",
        "#### 2. **Poor Minority Class Detection**\n",
        "\n",
        "* The model becomes **biased** toward the majority class.\n",
        "* It may **underfit** the minority class, treating it as noise or rare outliers.\n",
        "\n",
        "#### 3. **Incorrect Threshold-Based Decisions**\n",
        "\n",
        "* Logistic regression outputs probabilities. The default classification threshold (usually 0.5) may not be optimal in imbalanced cases.\n",
        "* A model may never predict probabilities high enough for the minority class to be labeled as positive.\n",
        "\n",
        "#### 4. **Skewed Performance Metrics**\n",
        "\n",
        "* Metrics like **accuracy** can be high while **recall, precision, or F1 score** for the minority class are very low.\n",
        "* **Confusion matrix** helps highlight these issues.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Address Class Imbalance:\n",
        "\n",
        "| Method                                                 | Description                                                                 |\n",
        "| ------------------------------------------------------ | --------------------------------------------------------------------------- |\n",
        "| **Resampling (Oversampling/Undersampling)**            | Balance classes by adding copies of minority or removing majority           |\n",
        "| **SMOTE (Synthetic Minority Over-sampling Technique)** | Creates synthetic data points for the minority class                        |\n",
        "| **Use of Class Weights**                               | Penalize the model more for misclassifying the minority class               |\n",
        "| **Adjusting Threshold**                                | Lower the classification threshold to favor minority detection              |\n",
        "| **Use Better Metrics**                                 | Use **F1-score**, **precision**, **recall**, or **AUC** instead of accuracy |\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "If a fraud detection model only labels transactions as \"not fraud\" due to class imbalance, it can miss nearly all actual frauds—despite high accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SVyQ_OfsTJUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "**Hyperparameter tuning** is the process of finding the **best set of hyperparameters** for a model to improve its performance on unseen data.\n",
        "\n",
        "In **logistic regression**, hyperparameters are not learned from the data but are set **before training**, and they significantly affect the model’s **accuracy**, **generalization**, and **robustness**.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Hyperparameters in Logistic Regression:\n",
        "\n",
        "| Hyperparameter       | Description                                                                                            |\n",
        "| -------------------- | ------------------------------------------------------------------------------------------------------ |\n",
        "| **C (Inverse of λ)** | Controls the **strength of regularization**. Smaller C = stronger penalty.                             |\n",
        "| **penalty**          | Type of regularization: `\"l1\"` (Lasso), `\"l2\"` (Ridge), `\"elasticnet\"`, or `\"none\"`                    |\n",
        "| **solver**           | Algorithm to use for optimization (e.g., `\"liblinear\"`, `\"saga\"`, `\"newton-cg\"`)                       |\n",
        "| **max\\_iter**        | Maximum number of iterations for convergence. Needed when the dataset is large or convergence is slow. |\n",
        "| **class\\_weight**    | Balances class imbalance (`\"balanced\"` or user-defined dictionary).                                    |\n",
        "\n",
        "---\n",
        "\n",
        "### Why Hyperparameter Tuning is Important:\n",
        "\n",
        "* **Improves model performance** by finding the best trade-off between bias and variance.\n",
        "* **Prevents overfitting** or **underfitting**.\n",
        "* Helps in **handling class imbalance** (e.g., through `class_weight`).\n",
        "* Selects the most appropriate solver and regularization strategy.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Methods for Hyperparameter Tuning:\n",
        "\n",
        "| Method                    | Description                                                   |\n",
        "| ------------------------- | ------------------------------------------------------------- |\n",
        "| **Grid Search**           | Tries all combinations from a defined parameter grid          |\n",
        "| **Random Search**         | Randomly samples combinations; faster than full grid search   |\n",
        "| **Bayesian Optimization** | Uses past results to decide the next set of parameters to try |\n",
        "| **Cross-Validation**      | Combined with above methods to ensure reliable evaluation     |\n",
        "\n",
        "---\n",
        "\n",
        "### Example Using GridSearchCV (Scikit-Learn):\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "JE0BgKgBTXfp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. What Are Different Solvers in Logistic Regression? Which One Should Be Used?\n",
        "\n",
        "In logistic regression (especially with **scikit-learn** in Python), the **solver** is the algorithm used to optimize the model’s cost function. Different solvers have different strengths depending on the **dataset size**, **regularization type**, and **sparsity** of features.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Solvers in Scikit-learn's Logistic Regression:\n",
        "\n",
        "| Solver      | Type of Optimization                            | Supports L1? | Supports L2? | Suitable For                                                 |\n",
        "| ----------- | ----------------------------------------------- | ------------ | ------------ | ------------------------------------------------------------ |\n",
        "| `liblinear` | Coordinate Descent (used in SVM)                | Yes          | Yes          | Small to medium datasets, binary or multiclass (one-vs-rest) |\n",
        "| `newton-cg` | Newton’s Method                                 | No           | Yes          | Large datasets, multiclass (softmax), L2 regularization      |\n",
        "| `lbfgs`     | Quasi-Newton (BFGS)                             | No           | Yes          | Large datasets, good default choice                          |\n",
        "| `sag`       | Stochastic Average Gradient                     | No           | Yes          | Large datasets, best for sparse data                         |\n",
        "| `saga`      | Stochastic Average Gradient with support for L1 | Yes          | Yes          | Large datasets, supports elastic-net and sparse data         |\n",
        "\n",
        "---\n",
        "\n",
        "### When to Use Which Solver?\n",
        "\n",
        "| Scenario                                   | Recommended Solver   |\n",
        "| ------------------------------------------ | -------------------- |\n",
        "| **Small dataset**                          | `liblinear`          |\n",
        "| **Multiclass classification with softmax** | `newton-cg`, `lbfgs` |\n",
        "| **L1 regularization (Lasso)**              | `liblinear`, `saga`  |\n",
        "| **Elastic Net regularization**             | `saga`               |\n",
        "| **Sparse or large-scale data**             | `sag` or `saga`      |\n",
        "| **Good general-purpose choice**            | `lbfgs`              |\n",
        "\n",
        "---\n",
        "\n",
        "### Important Notes:\n",
        "\n",
        "* **`liblinear`** supports only **L1 and L2**, not Elastic Net.\n",
        "* **`saga`** is the most **flexible solver**, supporting all penalties (`l1`, `l2`, `elasticnet`, `none`).\n",
        "* Always pair the solver with appropriate **regularization** settings; not all solvers support all penalties.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p1SQ1hxaTmQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. How is Logistic Regression Extended for Multiclass Classification?\n",
        "\n",
        "By default, **logistic regression** is a **binary classifier**—it predicts one of two classes (e.g., 0 or 1). To handle **multiclass classification** (three or more classes), logistic regression is **extended** using strategies like:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **One-vs-Rest (OvR)** — *default in scikit-learn*\n",
        "\n",
        "* Also called **One-vs-All**.\n",
        "* For $K$ classes, train **K separate binary classifiers**.\n",
        "* Each classifier predicts the probability that a sample belongs to one specific class vs. all others.\n",
        "* The class with the **highest probability** is chosen.\n",
        "\n",
        "**Example**:\n",
        "For classes A, B, C:\n",
        "\n",
        "* Model 1: A vs (B+C)\n",
        "* Model 2: B vs (A+C)\n",
        "* Model 3: C vs (A+B)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Multinomial (Softmax Regression)**\n",
        "\n",
        "* Instead of training multiple binary classifiers, it **directly estimates probabilities for all classes** in one model using the **softmax function**.\n",
        "* More **computationally efficient and accurate** when classes are **mutually exclusive**.\n",
        "\n",
        "**Softmax Function**:\n",
        "\n",
        "$$\n",
        "P(y = k | x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n",
        "$$\n",
        "\n",
        "* Typically used with solvers like **`lbfgs`** or **`newton-cg`**.\n",
        "* Scikit-learn: set `multi_class='multinomial'` and use an appropriate solver.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **One-vs-One (OvO)** *(less common in logistic regression)*\n",
        "\n",
        "* Trains a classifier for **every pair of classes**.\n",
        "* For $K$ classes, requires $\\frac{K(K-1)}{2}$ classifiers.\n",
        "* Class with the most “wins” across classifiers is selected.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison Table:\n",
        "\n",
        "| Method      | # of Models | Pros                         | Cons                                     |\n",
        "| ----------- | ----------- | ---------------------------- | ---------------------------------------- |\n",
        "| One-vs-Rest | K           | Simple, easy to implement    | May be less accurate than softmax        |\n",
        "| Multinomial | 1           | More accurate, uses all data | Requires suitable solver                 |\n",
        "| One-vs-One  | K(K-1)/2    | Precise for some algorithms  | Not commonly used in logistic regression |\n",
        "\n",
        "---\n",
        "\n",
        "### In Scikit-learn:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Multinomial example\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mf1OgUrPTyxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. What Are the Advantages and Disadvantages of Logistic Regression?\n",
        "\n",
        "#### **Advantages:**\n",
        "\n",
        "1. **Simple and Interpretable**\n",
        "\n",
        "   * Easy to implement and understand.\n",
        "   * Coefficients can be interpreted to understand the relationship between features and the outcome.\n",
        "\n",
        "2. **Efficient and Fast**\n",
        "\n",
        "   * Requires less computation than complex models.\n",
        "   * Scales well with smaller datasets.\n",
        "\n",
        "3. **Probabilistic Output**\n",
        "\n",
        "   * Provides class probabilities, not just labels—useful for ranking and threshold-based decisions.\n",
        "\n",
        "4. **Works Well with Linearly Separable Classes**\n",
        "\n",
        "   * Performs well when the classes can be separated by a linear boundary.\n",
        "\n",
        "5. **Regularization Support**\n",
        "\n",
        "   * Built-in support for L1, L2, and Elastic Net regularization helps avoid overfitting.\n",
        "\n",
        "6. **Less Prone to Overfitting (with regularization)**\n",
        "\n",
        "   * Compared to high-complexity models like decision trees or neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Disadvantages:**\n",
        "\n",
        "1. **Assumes Linear Decision Boundary**\n",
        "\n",
        "   * Not suitable for complex non-linear relationships unless feature engineering is done.\n",
        "\n",
        "2. **Sensitive to Outliers**\n",
        "\n",
        "   * Outliers can distort the model’s predictions significantly.\n",
        "\n",
        "3. **Requires Careful Feature Scaling (for regularization)**\n",
        "\n",
        "   * Regularization is sensitive to feature scale; standardization is often required.\n",
        "\n",
        "4. **Limited Expressiveness**\n",
        "\n",
        "   * May underperform on complex datasets where more flexible models (e.g., random forest, SVM, neural nets) do better.\n",
        "\n",
        "5. **Needs Balanced Data or Adjustments**\n",
        "\n",
        "   * Performs poorly on imbalanced datasets unless you use class weighting or sampling techniques.\n",
        "\n",
        "6. **Requires Independent Observations**\n",
        "\n",
        "   * Assumes that each observation is independent; not suited for time-series or grouped data without modifications.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Advantages                          | Disadvantages                               |\n",
        "| ----------------------------------- | ------------------------------------------- |\n",
        "| Easy to implement and interpret     | Assumes linear relationship (log-odds)      |\n",
        "| Fast training and prediction        | Poor with non-linear boundaries             |\n",
        "| Outputs probabilities               | Sensitive to outliers                       |\n",
        "| Supports regularization             | Requires feature scaling for regularization |\n",
        "| Works well on small, clean datasets | May fail on complex or imbalanced datasets  |\n",
        "\n"
      ],
      "metadata": {
        "id": "i1Atz4WUUAPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. What Are Some Use Cases of Logistic Regression?\n",
        "\n",
        "Logistic Regression is widely used for **binary and multiclass classification tasks**, especially when model interpretability, efficiency, and probability estimation are important.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Use Cases:\n",
        "\n",
        "#### 1. **Medical Diagnosis**\n",
        "\n",
        "* **Example**: Predicting whether a patient has a disease (Yes/No) based on symptoms, lab results, or images.\n",
        "* **Why**: Simple and interpretable—doctors can understand feature importance.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Spam Detection**\n",
        "\n",
        "* **Example**: Classifying emails as spam or not spam.\n",
        "* **Why**: Works well with high-dimensional, sparse text data (often combined with Naive Bayes or SVM).\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Credit Scoring / Loan Approval**\n",
        "\n",
        "* **Example**: Predicting whether a loan applicant will default or repay.\n",
        "* **Why**: Financial institutions prefer interpretable models for risk assessment.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Customer Churn Prediction**\n",
        "\n",
        "* **Example**: Will a customer cancel their subscription?\n",
        "* **Why**: Helps in retention strategies by identifying at-risk users.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Marketing Campaign Response**\n",
        "\n",
        "* **Example**: Will a customer respond to a marketing email or not?\n",
        "* **Why**: Logistic regression can model likelihood and guide targeted campaigns.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **Fraud Detection**\n",
        "\n",
        "* **Example**: Predicting whether a transaction is fraudulent.\n",
        "* **Why**: High precision and recall matter; logistic regression can serve as a baseline or part of an ensemble.\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Click-Through Rate (CTR) Prediction**\n",
        "\n",
        "* **Example**: Will a user click on an ad?\n",
        "* **Why**: Logistic regression is used in large-scale ad-tech systems due to its scalability and speed.\n",
        "\n",
        "---\n",
        "\n",
        "#### 8. **Voting Prediction**\n",
        "\n",
        "* **Example**: Will a person vote for a certain party based on demographics and opinion polls?\n",
        "* **Why**: Useful in political science and survey analytics.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Domain        | Use Case Example            | Target Variable       |\n",
        "| ------------- | --------------------------- | --------------------- |\n",
        "| Healthcare    | Disease diagnosis           | Has disease (Yes/No)  |\n",
        "| Finance       | Loan approval / credit risk | Default (Yes/No)      |\n",
        "| Marketing     | Campaign response           | Will respond (Yes/No) |\n",
        "| E-commerce    | Purchase prediction         | Will buy (Yes/No)     |\n",
        "| Telecom       | Churn prediction            | Will churn (Yes/No)   |\n",
        "| Cybersecurity | Fraud or spam detection     | Fraudulent (Yes/No)   |\n",
        "\n"
      ],
      "metadata": {
        "id": "PQbWRfYMULjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. What Is the Difference Between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Both **Logistic Regression** and **Softmax Regression** are used for **classification**, but they differ in terms of **number of output classes** and **how they model class probabilities**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Logistic Regression (Binary Classification)\n",
        "\n",
        "* Used when the **target variable has only two classes** (e.g., 0 and 1).\n",
        "* Uses the **sigmoid function** to output a probability between 0 and 1.\n",
        "* Decision is made by comparing predicted probability to a **threshold** (usually 0.5).\n",
        "\n",
        "**Sigmoid Function:**\n",
        "\n",
        "$$\n",
        "P(y = 1 | x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Softmax Regression (Multinomial Logistic Regression)\n",
        "\n",
        "* Generalization of logistic regression to **multiclass problems**.\n",
        "* Used when the **target variable has more than two classes** (e.g., A, B, C).\n",
        "* Uses the **softmax function** to assign probabilities to **all classes**, ensuring they sum to 1.\n",
        "\n",
        "**Softmax Function:**\n",
        "\n",
        "$$\n",
        "P(y = k | x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}, \\quad \\text{for } k = 1, \\dots, K\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 Key Differences:\n",
        "\n",
        "| Feature             | Logistic Regression         | Softmax Regression                       |\n",
        "| ------------------- | --------------------------- | ---------------------------------------- |\n",
        "| Use Case            | Binary classification       | Multiclass classification                |\n",
        "| Activation Function | Sigmoid                     | Softmax                                  |\n",
        "| Output              | Probability for one class   | Probabilities for all classes            |\n",
        "| Decision            | Threshold-based (e.g., 0.5) | Argmax of predicted probabilities        |\n",
        "| Loss Function       | Binary cross-entropy        | Categorical cross-entropy (softmax loss) |\n",
        "\n",
        "---\n",
        "\n",
        "### Example:\n",
        "\n",
        "If you're predicting **email type**:\n",
        "\n",
        "* Logistic Regression: \"Spam\" vs \"Not Spam\"\n",
        "* Softmax Regression: \"Spam\", \"Promotional\", \"Updates\", \"Social\", etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wdthv-VaUZsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 19. How Do We Choose Between One-vs-Rest (OvR) and Softmax for Multiclass Classification?\n",
        "\n",
        "Both **One-vs-Rest (OvR)** and **Softmax (Multinomial Logistic Regression)** are strategies to extend logistic regression to **multiclass problems**, but they differ in performance, training style, and suitability depending on the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 One-vs-Rest (OvR)\n",
        "\n",
        "* Trains **one binary classifier per class**.\n",
        "* For $K$ classes, builds **K separate classifiers**: each class vs. all others.\n",
        "* At prediction time, selects the class with the **highest predicted probability**.\n",
        "\n",
        "**When to Choose OvR:**\n",
        "\n",
        "| Criteria                       | Explanation                                   |\n",
        "| ------------------------------ | --------------------------------------------- |\n",
        "| Small or medium-sized datasets | OvR models are simple and fast to train       |\n",
        "| Classes are well-separated     | Works well when classes don’t overlap heavily |\n",
        "| Interpretability is important  | Easier to analyze individual class boundaries |\n",
        "| Imbalanced class distributions | Individual models can be tuned per class      |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Softmax (Multinomial Logistic Regression)\n",
        "\n",
        "* Trains **one model for all classes at once**, optimizing a single loss function using the **softmax function**.\n",
        "* Outputs a **probability distribution over all classes**.\n",
        "* Typically more **mathematically elegant and efficient** for large datasets.\n",
        "\n",
        "**When to Choose Softmax:**\n",
        "\n",
        "| Criteria                         | Explanation                                      |\n",
        "| -------------------------------- | ------------------------------------------------ |\n",
        "| Large datasets                   | More efficient and accurate overall              |\n",
        "| Mutually exclusive classes       | Softmax directly models class probabilities      |\n",
        "| Accuracy across all classes      | Often outperforms OvR on tightly related classes |\n",
        "| Need for probability calibration | Probabilities are more reliable and normalized   |\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table:\n",
        "\n",
        "| Feature                   | One-vs-Rest (OvR)        | Softmax (Multinomial)          |\n",
        "| ------------------------- | ------------------------ | ------------------------------ |\n",
        "| Number of models          | K (one per class)        | 1 unified model                |\n",
        "| Handles class overlap     | Not as well              | Better                         |\n",
        "| Speed (small datasets)    | Faster and simpler       | Slightly heavier               |\n",
        "| Accuracy (large datasets) | May underperform         | Typically better               |\n",
        "| Implementation ease       | Simple in most libraries | Also supported in scikit-learn |\n",
        "| Probabilities             | May not sum to 1         | Always sum to 1                |\n",
        "\n",
        "---\n",
        "\n",
        "### In Scikit-learn:\n",
        "\n",
        "```python\n",
        "# OvR (default)\n",
        "LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Softmax\n",
        "LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "vWAh9IvUUoRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. How Do We Interpret Coefficients in Logistic Regression?\n",
        "\n",
        "In logistic regression, coefficients represent the effect of **one unit change in a feature (independent variable)** on the **log-odds** of the target variable being 1, holding all other features constant.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Log-Odds Interpretation**\n",
        "\n",
        "* The logistic regression equation is:\n",
        "\n",
        "  $$\n",
        "  \\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "  $$\n",
        "* Here, $\\beta_i$ is the change in the **log-odds** of the outcome for a one-unit increase in $x_i$.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Odds Ratio (Exponentiated Coefficients)**\n",
        "\n",
        "* Taking the exponential of a coefficient gives the **odds ratio**:\n",
        "\n",
        "  $$\n",
        "  \\text{Odds Ratio} = e^{\\beta_i}\n",
        "  $$\n",
        "* Interpretation:\n",
        "\n",
        "  * If $e^{\\beta_i} = 1$: no effect on odds\n",
        "  * If $e^{\\beta_i} > 1$: odds increase with an increase in the feature\n",
        "  * If $e^{\\beta_i} < 1$: odds decrease with an increase in the feature\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Example Interpretation**\n",
        "\n",
        "| Coefficient $\\beta_i$ | Odds Ratio $e^{\\beta_i}$ | Interpretation                       |\n",
        "| --------------------- | ------------------------ | ------------------------------------ |\n",
        "| 0                     | 1                        | No impact                            |\n",
        "| 0.693                 | 2                        | A one-unit increase doubles the odds |\n",
        "| -0.693                | 0.5                      | A one-unit increase halves the odds  |\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Cautions in Interpretation:**\n",
        "\n",
        "* **Coefficients are in terms of log-odds**, not probability.\n",
        "* Must **standardize/scale** features if they are on different scales for meaningful comparison.\n",
        "* **Interactions and non-linear relationships** are not captured unless explicitly modeled.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Categorical Variables:**\n",
        "\n",
        "* For dummy variables (e.g., gender = 1 if male, 0 if female), the coefficient shows how the log-odds change **relative to the reference category**.\n",
        "\n",
        "---\n",
        "\n",
        "### Python Example (Scikit-learn):\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "model = LogisticRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Coefficients\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "# Odds Ratios\n",
        "print(\"Odds Ratios:\", np.exp(model.coef_))\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "kiIxd5uxU2n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical**"
      ],
      "metadata": {
        "id": "mpD7eloRVHyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''1 .Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCxK2YqOVQcD",
        "outputId": "09dd8cf7-5bed-4684-beeb-ec4815655bdb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' 2 .  Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy.'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTImyRSOVp4P",
        "outputId": "56ce2fcd-1071-494d-e74d-1c967acb7480"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''3 . Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients.'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization (default penalty)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print model accuracy and coefficients\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(np.round(model.coef_, 4))  # Rounded for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOAkl1VpV_zk",
        "outputId": "b9135051-e974-4e65-8940-76211c847482"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.00\n",
            "\n",
            "Model Coefficients:\n",
            "[[-0.3935  0.9625 -2.3751 -0.9987]\n",
            " [ 0.5084 -0.2548 -0.213  -0.7757]\n",
            " [-0.115  -0.7077  2.5881  1.7745]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''4 . Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',          # Required for elasticnet\n",
        "    l1_ratio=0.5,           # 0 = Ridge, 1 = Lasso, 0.5 = Elastic Net\n",
        "    C=1.0,\n",
        "    max_iter=500\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(np.round(model.coef_, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgGxrpLAWTtR",
        "outputId": "6cdf83cd-272e-408f-b42f-2d67b6126c05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.00\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.2553  1.7346 -2.4314 -0.6189]\n",
            " [ 0.      0.      0.     -0.5081]\n",
            " [-1.0108 -1.2094  2.6471  2.1074]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''5 . Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'.'''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target  # 3 classes: 0, 1, 2\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression using One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Multiclass Classification Accuracy (OvR): {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients (OvR):\")\n",
        "print(np.round(model.coef_, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EeQLiewWmC0",
        "outputId": "7ca9cdf8-3531-403f-c57b-bbd75939cc35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Classification Accuracy (OvR): 1.00\n",
            "\n",
            "Model Coefficients (OvR):\n",
            "[[ 0.3711  1.4097 -2.1521 -0.9547]\n",
            " [ 0.494  -1.589   0.4372 -1.1119]\n",
            " [-1.559  -1.5889  2.3987  2.1556]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''6 . Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy.'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define parameter grid and model\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Use solver that supports both l1 and l2\n",
        "model = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "\n",
        "# Step 4: GridSearchCV\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print best params and test accuracy\n",
        "print(\"Best Parameters from GridSearchCV:\")\n",
        "print(grid.best_params_)\n",
        "print(f\"\\nTest Accuracy using Best Parameters: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG8siwDPWyix",
        "outputId": "25913cad-7f43-4358-b17b-886eec75b6a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters from GridSearchCV:\n",
            "{'C': 10, 'penalty': 'l1'}\n",
            "\n",
            "Test Accuracy using Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''7 . Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy.'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "\n",
        "# Step 3: Define Stratified K-Fold cross-validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Print accuracy for each fold and average\n",
        "print(\"Accuracy for each fold:\", np.round(scores, 4))\n",
        "print(f\"Average Accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDRD7tgW-31",
        "outputId": "483e78fe-da15-48ba-a454-6124a0761d10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for each fold: [0.9667 1.     0.9    0.9333 1.    ]\n",
            "Average Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''8 . Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV\n",
        "# Make sure to replace 'your_dataset.csv' with the actual filename\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Step 2: Split features and target\n",
        "# Replace 'target_column' with your actual target column name\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "Ry16n3opXqsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''9 . Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy.'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Step 1: Load dataset (or replace with CSV file)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Step 4: Define parameter distribution\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 10),  # Try values from 0.01 to 10\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'l1_ratio': [None, 0.3, 0.5, 0.7, 1.0]  # Only used if penalty='elasticnet'\n",
        "}\n",
        "\n",
        "# Step 5: Randomized search\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict and evaluate\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print results\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(random_search.best_params_)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX7934GzXsPg",
        "outputId": "747ac521-09c5-46b5-a9ec-312289ee2491"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'C': np.float64(1.5699452033620265), 'l1_ratio': 0.5, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "\n",
            "Test Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "10 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.96666667 0.975      0.95       0.96666667 0.325\n",
            " 0.95       0.96666667 0.96666667 0.95833333        nan 0.96666667\n",
            " 0.96666667 0.96666667 0.95       0.96666667 0.95833333 0.96666667\n",
            " 0.96666667 0.95      ]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''10 . Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target  # 3 classes\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create Logistic Regression model\n",
        "base_model = LogisticRegression(solver='liblinear', max_iter=200)\n",
        "\n",
        "# Step 4: Wrap it with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"One-vs-One (OvO) Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3sL70kTX-GF",
        "outputId": "794f3432-2360-4b87-87cd-601af873fbab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One (OvO) Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''11 . Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification.'''\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary: 0 (malignant), 1 (benign)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names,\n",
        "            yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title(f'Confusion Matrix (Accuracy: {accuracy:.2f})')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "_e7CcpQTYN5K",
        "outputId": "5872d7d9-9df5-4fc7-fa1b-66135b442673"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''12 .Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load binary dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train logistic regression\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvtKEMrcYbs3",
        "outputId": "6b207e83-ae2f-430e-c609-a5ea0607426b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.97      0.91      0.94        43\n",
            "      benign       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''13 .  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance'''\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Generate imbalanced dataset (90% of class 0, 10% of class 1)\n",
        "X, y = make_classification(n_samples=1000, n_features=10,\n",
        "                           n_classes=2, weights=[0.9, 0.1],\n",
        "                           n_informative=5, n_redundant=2,\n",
        "                           random_state=42)\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train without class weights (default)\n",
        "model_default = LogisticRegression(solver='liblinear')\n",
        "model_default.fit(X_train, y_train)\n",
        "y_pred_default = model_default.predict(X_test)\n",
        "\n",
        "print(\"Without Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_default))\n",
        "\n",
        "# Step 4: Train with class_weight='balanced'\n",
        "model_weighted = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "\n",
        "print(\"\\nWith class_weight='balanced':\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT7jE47VYqBG",
        "outputId": "993d88fc-5b3e-4d13-b0b1-07dbbe6536f5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Class Weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.99      0.95       179\n",
            "           1       0.83      0.24      0.37        21\n",
            "\n",
            "    accuracy                           0.92       200\n",
            "   macro avg       0.88      0.62      0.66       200\n",
            "weighted avg       0.91      0.92      0.89       200\n",
            "\n",
            "\n",
            "With class_weight='balanced':\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.82      0.89       179\n",
            "           1       0.35      0.81      0.49        21\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.66      0.82      0.69       200\n",
            "weighted avg       0.91      0.82      0.85       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''14 . Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance.'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "# If you're using seaborn version, use: sns.load_dataset(\"titanic\")\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
        "\n",
        "# Step 2: Select features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "df = df[features + [target]]\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Step 4: Encode categorical variables\n",
        "label_encoders = {}\n",
        "for col in ['Sex', 'Embarked']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le  # store encoders if needed later\n",
        "\n",
        "# Step 5: Split data\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, r_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "1UmPDiu1Y5uf",
        "outputId": "8be4d8ae-ade0-4d45-df69-7abcbcda2f31"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-14-4074032737.py, line 33)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-14-4074032737.py\"\u001b[0;36m, line \u001b[0;32m33\u001b[0m\n\u001b[0;31m    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, r_\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''15 . Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------------------- Without Scaling --------------------\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -------------------- With Standardization --------------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# -------------------- Print Results --------------------\n",
        "print(f\"Accuracy without Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with Standardization: {accuracy_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jOC9-m1ZFkk",
        "outputId": "b9d91327-e0ec-43ca-9dd9-cb385416b92a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 0.9561\n",
            "Accuracy with Standardization: 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''16 . Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities\n",
        "y_proba = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
        "\n",
        "# Step 5: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFN4or3ZSQ5",
        "outputId": "9086cf12-30c6-4b97-b591-5d839cf2f7dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''17 .Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression with C=0.5\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILKJaV3xZeT4",
        "outputId": "8baf6498-9e36-4ed8-a94b-32f1326d29b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with C=0.5: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''18 .Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients'''\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train logistic regression\n",
        "model = LogisticRegression(max_iter=200, solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Extract feature importances\n",
        "coefficients = model.coef_[0]  # shape: (n_features,)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': coefficients,\n",
        "    'Importance': abs(coefficients)\n",
        "})\n",
        "\n",
        "# Step 5: Sort by importance and display top features\n",
        "top_features = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top Important Features based on Logistic Regression Coefficients:\\n\")\n",
        "print(top_features.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-rwbKsXZq3I",
        "outputId": "7f310ce4-296b-44ca-9481-c06f751e3c8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Important Features based on Logistic Regression Coefficients:\n",
            "\n",
            "                 Feature  Coefficient  Importance\n",
            "0            mean radius     2.132484    2.132484\n",
            "26       worst concavity    -1.617969    1.617969\n",
            "11         texture error     1.442984    1.442984\n",
            "20          worst radius     1.232150    1.232150\n",
            "25     worst compactness    -1.208985    1.208985\n",
            "28        worst symmetry    -0.742764    0.742764\n",
            "6         mean concavity    -0.651940    0.651940\n",
            "27  worst concave points    -0.615251    0.615251\n",
            "5       mean compactness    -0.415569    0.415569\n",
            "21         worst texture    -0.404581    0.404581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''19 . Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score.'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary classes: 0 and 1\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4LvV_PIZ10l",
        "outputId": "47fefacb-6ce7-465f-eb40-d0f82c340037"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''20 . Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load binary dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary: 0 (malignant), 1 (benign)\n",
        "\n",
        "# Step 2: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities for positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute precision-recall pairs and average precision\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Step 6: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}', color='blue')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve (Logistic Regression)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "25a7HA-faArh",
        "outputId": "0f0ed952-46d7-491f-dc05-5eb0ed7c1fe4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''21 .Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "# Step 4: Train and evaluate model for each solver\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=300)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[solver] = acc\n",
        "    except Exception as e:\n",
        "        results[solver] = f\"Failed: {e}\"\n",
        "\n",
        "# Step 5: Print accuracy for each solver\n",
        "print(\"Solver Comparison (Accuracy on Test Set):\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver}: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xcon2qAVaMfu",
        "outputId": "2264fe66-8795-4acc-8038-8a540d64e953"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver Comparison (Accuracy on Test Set):\n",
            "liblinear: 0.956140350877193\n",
            "saga: 0.956140350877193\n",
            "lbfgs: 0.956140350877193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''22 . Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # Binary: 0 (malignant), 1 (benign)\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train logistic regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict labels\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tdsw8Z0aYqP",
        "outputId": "32d1079c-4d0c-4c9c-873f-833938fb9e74"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''23 . Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 4: Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "# Step 5: Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print comparison\n",
        "print(f\"Accuracy on Raw Data:         {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTDr-0Ukam40",
        "outputId": "812c7d14-3db7-4fac-b870-d4e5fe730016"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data:         0.9561\n",
            "Accuracy on Standardized Data: 0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''24 . Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation'''\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Set up Logistic Regression and parameter grid for C\n",
        "model = LogisticRegression(max_iter=300, solver='liblinear')\n",
        "param_grid = {'C': np.logspace(-3, 3, 10)}  # Test C values from 0.001 to 1000\n",
        "\n",
        "# Step 4: Grid search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"Best C (Regularization Strength):\", grid_search.best_params_['C'])\n",
        "print(f\"Test Accuracy with Best C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WLI98qIa0U3",
        "outputId": "976d6fc5-c8e4-4ec9-ec40-1a062c666d20"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C (Regularization Strength): 10.0\n",
            "Test Accuracy with Best C: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''25 . Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions'''\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train logistic regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save model to disk\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "print(\"Model saved as 'logistic_model.joblib'.\")\n",
        "\n",
        "# Step 5: Load model from disk\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "\n",
        "# Step 6: Make predictions with loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate loaded model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85QIps55bBmR",
        "outputId": "d1f737fb-b78a-42b8-c4e5-d7af49358243"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'logistic_model.joblib'.\n",
            "Accuracy of loaded model: 0.9561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    }
  ]
}